{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0Ee8tDwErgC"
      },
      "source": [
        "# Agentic RAG System - Multi-Source Intelligence Platform\n",
        "\n",
        "\"\"\"\n",
        "# ğŸ¤– Agentic RAG System - Multi-Source Intelligence Platform\n",
        "\n",
        "This notebook demonstrates an advanced Retrieval-Augmented Generation (RAG) system\n",
        "with autonomous agents that can:\n",
        "- Decompose complex queries into research steps\n",
        "- Retrieve information from multiple sources\n",
        "- Analyze blockchain data via Etherscan API\n",
        "- Perform web research using OpenAI\n",
        "- Synthesize comprehensive responses\n",
        "\n",
        "## Features:\n",
        "- ğŸ§  Multi-agent architecture with specialized roles\n",
        "- ğŸ“Š Real-time data integration (Crypto + Web)\n",
        "- âš¡ Autonomous decision-making and coordination\n",
        "- ğŸ” Vector-based semantic search\n",
        "- ğŸ“ˆ Confidence scoring and source attribution\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtT2Ft8ebH59",
        "outputId": "dc53dd92-5913-4a84-cc88-72dc183e17c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q requests\n",
        "!pip install -q beautifulsoup4\n",
        "!pip install -q pandas\n",
        "!pip install -q numpy\n",
        "!pip install -q chromadb\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-openai\n",
        "!pip install -q gradio\n",
        "!pip install -q plotly\n",
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMJhLNe30ImX",
        "outputId": "9affb7ea-53f5-4152-91f2-da0e2712b2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization libraries\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# AI and ML libraries\n",
        "import openai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "# Gradio for UI\n",
        "import gradio as gr\n",
        "\n",
        "print(\"ğŸš€ All packages installed successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KONY3kPLMyEY",
        "outputId": "19aeaed1-8a82-4af7-eb2d-05023dac6ef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… API keys configured successfully!\n"
          ]
        }
      ],
      "source": [
        "# API Keys (embedded directly as requested)\n",
        "OPENAI_API_KEY = 'test-openai'\n",
        "ETHERSCAN_API_KEY = 'test-etherscan'\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"âœ… API keys configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBa6NM8ZEyn_"
      },
      "source": [
        "# UTILITY CLASSES AND FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VuEPfQ1E-5NS"
      },
      "outputs": [],
      "source": [
        "class SystemLogger:\n",
        "    \"\"\"Enhanced logging system for agent activities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.logs = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log(self, agent_name: str, action: str, details: str = \"\", status: str = \"INFO\"):\n",
        "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "        log_entry = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"agent\": agent_name,\n",
        "            \"action\": action,\n",
        "            \"details\": details,\n",
        "            \"status\": status,\n",
        "            \"elapsed\": round(time.time() - self.start_time, 2)\n",
        "        }\n",
        "        self.logs.append(log_entry)\n",
        "\n",
        "        # Print formatted log\n",
        "        status_emoji = {\"INFO\": \"â„¹ï¸\", \"SUCCESS\": \"âœ…\", \"ERROR\": \"âŒ\", \"PROCESSING\": \"âš™ï¸\"}\n",
        "        print(f\"{status_emoji.get(status, 'ğŸ“')} [{timestamp}] {agent_name}: {action}\")\n",
        "        if details:\n",
        "            print(f\"   â””â”€ {details}\")\n",
        "\n",
        "    def get_summary(self):\n",
        "        return {\n",
        "            \"total_logs\": len(self.logs),\n",
        "            \"agents_active\": len(set(log[\"agent\"] for log in self.logs)),\n",
        "            \"total_time\": round(time.time() - self.start_time, 2),\n",
        "            \"logs\": self.logs\n",
        "        }\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Vector-based knowledge storage and retrieval\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.embeddings = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.documents = []\n",
        "        self.vectors = []\n",
        "        self.metadata = []\n",
        "\n",
        "        # Initialize with sample knowledge\n",
        "        self._initialize_knowledge()\n",
        "\n",
        "    def _initialize_knowledge(self):\n",
        "        \"\"\"Initialize with domain-specific knowledge\"\"\"\n",
        "        sample_docs = [\n",
        "            {\n",
        "                \"content\": \"Ethereum is a decentralized blockchain platform that enables smart contracts and decentralized applications (DApps). It uses a proof-of-stake consensus mechanism after the Merge upgrade.\",\n",
        "                \"source\": \"Ethereum Foundation\",\n",
        "                \"category\": \"blockchain\",\n",
        "                \"importance\": 0.9\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"DeFi (Decentralized Finance) refers to financial services built on blockchain technology, eliminating intermediaries. Total Value Locked (TVL) is a key metric measuring DeFi adoption.\",\n",
        "                \"source\": \"DeFi Research\",\n",
        "                \"category\": \"defi\",\n",
        "                \"importance\": 0.8\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"Gas fees on Ethereum represent the cost of computational resources needed to process transactions. They fluctuate based on network demand and congestion.\",\n",
        "                \"source\": \"Ethereum Documentation\",\n",
        "                \"category\": \"blockchain\",\n",
        "                \"importance\": 0.7\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"Layer 2 scaling solutions like Arbitrum, Optimism, and Polygon help reduce Ethereum's transaction costs and increase throughput while maintaining security.\",\n",
        "                \"source\": \"Layer 2 Research\",\n",
        "                \"category\": \"scaling\",\n",
        "                \"importance\": 0.8\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with language generation to produce more accurate and contextual responses.\",\n",
        "                \"source\": \"AI Research\",\n",
        "                \"category\": \"ai\",\n",
        "                \"importance\": 0.9\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for doc in sample_docs:\n",
        "            self.add_document(doc[\"content\"], doc[\"source\"], doc[\"category\"], doc[\"importance\"])\n",
        "\n",
        "    def add_document(self, content: str, source: str, category: str, importance: float = 0.5):\n",
        "        \"\"\"Add document to knowledge base\"\"\"\n",
        "        vector = self.embeddings.encode(content)\n",
        "\n",
        "        self.documents.append(content)\n",
        "        self.vectors.append(vector)\n",
        "        self.metadata.append({\n",
        "            \"source\": source,\n",
        "            \"category\": category,\n",
        "            \"importance\": importance,\n",
        "            \"added_at\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Semantic search in knowledge base\"\"\"\n",
        "        if not self.vectors:\n",
        "            return []\n",
        "\n",
        "        query_vector = self.embeddings.encode(query)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = []\n",
        "        for i, doc_vector in enumerate(self.vectors):\n",
        "            similarity = np.dot(query_vector, doc_vector) / (\n",
        "                np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)\n",
        "            )\n",
        "            similarities.append((similarity, i))\n",
        "\n",
        "        # Sort by similarity and return top-k\n",
        "        similarities.sort(reverse=True)\n",
        "        results = []\n",
        "\n",
        "        for similarity, idx in similarities[:top_k]:\n",
        "            if similarity > 0.3:  # Threshold for relevance\n",
        "                results.append({\n",
        "                    \"content\": self.documents[idx],\n",
        "                    \"similarity\": similarity,\n",
        "                    \"metadata\": self.metadata[idx]\n",
        "                })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7rSbnpYE2pA"
      },
      "source": [
        "# SPECIALIZED AGENT CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r8_qT3wN-xNk"
      },
      "outputs": [],
      "source": [
        "class PlannerAgent:\n",
        "    \"\"\"Intelligent query analysis and planning agent\"\"\"\n",
        "\n",
        "    def __init__(self, logger: SystemLogger):\n",
        "        self.logger = logger\n",
        "        self.name = \"Planner\"\n",
        "\n",
        "    def analyze_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze query complexity and create execution plan\"\"\"\n",
        "        self.logger.log(self.name, \"Analyzing query structure\", status=\"PROCESSING\")\n",
        "\n",
        "        # Detect query characteristics\n",
        "        crypto_keywords = ['ethereum', 'bitcoin', 'crypto', 'defi', 'blockchain', 'gas', 'tvl', 'dao', 'nft']\n",
        "        web_keywords = ['latest', 'current', 'recent', 'news', 'update', 'trend']\n",
        "        analysis_keywords = ['analyze', 'compare', 'correlation', 'trend', 'relationship']\n",
        "\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        plan = {\n",
        "            \"complexity\": \"low\",\n",
        "            \"requires_crypto_data\": any(keyword in query_lower for keyword in crypto_keywords),\n",
        "            \"requires_web_search\": any(keyword in query_lower for keyword in web_keywords),\n",
        "            \"requires_analysis\": any(keyword in query_lower for keyword in analysis_keywords),\n",
        "            \"estimated_steps\": 3,\n",
        "            \"agents_needed\": [\"retriever\"],\n",
        "            \"confidence\": 0.8\n",
        "        }\n",
        "\n",
        "        # Adjust complexity based on requirements\n",
        "        if plan[\"requires_crypto_data\"]:\n",
        "            plan[\"agents_needed\"].append(\"crypto\")\n",
        "            plan[\"estimated_steps\"] += 2\n",
        "\n",
        "        if plan[\"requires_web_search\"]:\n",
        "            plan[\"agents_needed\"].append(\"web\")\n",
        "            plan[\"estimated_steps\"] += 1\n",
        "\n",
        "        if plan[\"requires_analysis\"]:\n",
        "            plan[\"agents_needed\"].append(\"reasoner\")\n",
        "            plan[\"estimated_steps\"] += 2\n",
        "            plan[\"complexity\"] = \"high\"\n",
        "\n",
        "        if len(plan[\"agents_needed\"]) > 2:\n",
        "            plan[\"complexity\"] = \"high\" if plan[\"complexity\"] != \"high\" else \"complex\"\n",
        "\n",
        "        self.logger.log(self.name, f\"Plan created - Complexity: {plan['complexity']}\",\n",
        "                       f\"Agents needed: {', '.join(plan['agents_needed'])}\", \"SUCCESS\")\n",
        "\n",
        "        return plan\n",
        "\n",
        "class RetrieverAgent:\n",
        "    \"\"\"Knowledge base retrieval and semantic search agent\"\"\"\n",
        "\n",
        "    def __init__(self, knowledge_base: KnowledgeBase, logger: SystemLogger):\n",
        "        self.knowledge_base = knowledge_base\n",
        "        self.logger = logger\n",
        "        self.name = \"Retriever\"\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve relevant information from knowledge base\"\"\"\n",
        "        self.logger.log(self.name, \"Searching knowledge base\", status=\"PROCESSING\")\n",
        "\n",
        "        results = self.knowledge_base.search(query, top_k)\n",
        "\n",
        "        self.logger.log(self.name, f\"Retrieved {len(results)} relevant documents\",\n",
        "                       f\"Avg similarity: {np.mean([r['similarity'] for r in results]):.2f}\" if results else \"No matches found\",\n",
        "                       \"SUCCESS\")\n",
        "\n",
        "        return results\n",
        "\n",
        "class CryptoAgent:\n",
        "    \"\"\"Blockchain data analysis agent using Etherscan API\"\"\"\n",
        "\n",
        "    def __init__(self, logger: SystemLogger):\n",
        "        self.logger = logger\n",
        "        self.name = \"Crypto\"\n",
        "\n",
        "        self.api_key = ETHERSCAN_API_KEY\n",
        "        self.base_url = \"https://api.etherscan.io/api\"\n",
        "\n",
        "    def get_eth_price(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current Ethereum price\"\"\"\n",
        "        try:\n",
        "            url = f\"{self.base_url}?module=stats&action=ethprice&apikey={self.api_key}\"\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "\n",
        "            if data['status'] == '1':\n",
        "                return {\n",
        "                    \"usd_price\": float(data['result']['ethusd']),\n",
        "                    \"btc_price\": float(data['result']['ethbtc']),\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "        except Exception as e:\n",
        "            self.logger.log(self.name, f\"Error fetching ETH price: {str(e)}\", status=\"ERROR\")\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def get_gas_tracker(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current gas prices\"\"\"\n",
        "        try:\n",
        "            url = f\"{self.base_url}?module=gastracker&action=gasoracle&apikey={self.api_key}\"\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "\n",
        "            if data['status'] == '1':\n",
        "                return {\n",
        "                    \"safe_gas\": int(data['result']['SafeGasPrice']),\n",
        "                    \"standard_gas\": int(data['result']['ProposeGasPrice']),\n",
        "                    \"fast_gas\": int(data['result']['FastGasPrice']),\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "        except Exception as e:\n",
        "            self.logger.log(self.name, f\"Error fetching gas prices: {str(e)}\", status=\"ERROR\")\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def analyze_blockchain_data(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze blockchain data based on query\"\"\"\n",
        "        self.logger.log(self.name, \"Fetching blockchain data\", status=\"PROCESSING\")\n",
        "\n",
        "        analysis = {\n",
        "            \"eth_price\": self.get_eth_price(),\n",
        "            \"gas_tracker\": self.get_gas_tracker(),\n",
        "            \"analysis_timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Generate insights\n",
        "        insights = []\n",
        "        if analysis[\"eth_price\"]:\n",
        "            insights.append(f\"Current ETH price: ${analysis['eth_price']['usd_price']:.2f}\")\n",
        "\n",
        "        if analysis[\"gas_tracker\"]:\n",
        "            gas_data = analysis[\"gas_tracker\"]\n",
        "            insights.append(f\"Gas prices - Safe: {gas_data['safe_gas']} Gwei, Standard: {gas_data['standard_gas']} Gwei, Fast: {gas_data['fast_gas']} Gwei\")\n",
        "\n",
        "            # Gas price analysis\n",
        "            if gas_data['standard_gas'] > 50:\n",
        "                insights.append(\"âš ï¸ High network congestion detected - consider using Layer 2 solutions\")\n",
        "            elif gas_data['standard_gas'] < 20:\n",
        "                insights.append(\"âœ… Low network congestion - good time for transactions\")\n",
        "\n",
        "        analysis[\"insights\"] = insights\n",
        "\n",
        "        self.logger.log(self.name, f\"Blockchain analysis complete\",\n",
        "                       f\"Generated {len(insights)} insights\", \"SUCCESS\")\n",
        "\n",
        "        return analysis\n",
        "\n",
        "class WebAgent:\n",
        "    \"\"\"Web research and real-time information gathering agent\"\"\"\n",
        "\n",
        "    def __init__(self, logger: SystemLogger):\n",
        "        self.logger = logger\n",
        "        self.name = \"Web\"\n",
        "        self.client = client\n",
        "\n",
        "    def search_web(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Perform web search using OpenAI for current information\"\"\"\n",
        "        self.logger.log(self.name, \"Performing web research\", status=\"PROCESSING\")\n",
        "\n",
        "        # Use OpenAI to simulate web research (in production, would use actual web APIs)\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            As a web research agent, provide current information about: {query}\n",
        "\n",
        "            Focus on:\n",
        "            1. Recent developments (last 3-6 months)\n",
        "            2. Key statistics and metrics\n",
        "            3. Notable trends or changes\n",
        "            4. Credible sources and references\n",
        "\n",
        "            Format as a structured research summary.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=800,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            research_summary = response.choices[0].message.content\n",
        "\n",
        "            # Simulate source attribution\n",
        "            sources = [\n",
        "                {\"url\": \"https://coinmarketcap.com\", \"title\": \"Crypto Market Data\", \"relevance\": 0.9},\n",
        "                {\"url\": \"https://defipulse.com\", \"title\": \"DeFi Analytics\", \"relevance\": 0.8},\n",
        "                {\"url\": \"https://ethereum.org\", \"title\": \"Ethereum Foundation\", \"relevance\": 0.7}\n",
        "            ]\n",
        "\n",
        "            result = {\n",
        "                \"summary\": research_summary,\n",
        "                \"sources\": sources,\n",
        "                \"search_timestamp\": datetime.now().isoformat(),\n",
        "                \"confidence\": 0.8\n",
        "            }\n",
        "\n",
        "            self.logger.log(self.name, f\"Web research complete\",\n",
        "                           f\"Analyzed {len(sources)} sources\", \"SUCCESS\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.log(self.name, f\"Web research error: {str(e)}\", status=\"ERROR\")\n",
        "            return {\"summary\": \"Web research unavailable\", \"sources\": [], \"confidence\": 0.0}\n",
        "\n",
        "class ReasonerAgent:\n",
        "    \"\"\"Logical reasoning and analysis agent\"\"\"\n",
        "\n",
        "    def __init__(self, logger: SystemLogger):\n",
        "        self.logger = logger\n",
        "        self.name = \"Reasoner\"\n",
        "        self.client = client\n",
        "\n",
        "    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Perform logical analysis on gathered data\"\"\"\n",
        "        self.logger.log(self.name, \"Performing logical analysis\", status=\"PROCESSING\")\n",
        "\n",
        "        try:\n",
        "            # Construct analysis prompt\n",
        "            prompt = f\"\"\"\n",
        "            As an expert analyst, analyze the following data and provide insights:\n",
        "\n",
        "            Query: {data.get('query', 'N/A')}\n",
        "\n",
        "            Knowledge Base Results: {len(data.get('kb_results', []))} documents found\n",
        "            Crypto Data: {bool(data.get('crypto_data', {}))}\n",
        "            Web Research: {bool(data.get('web_data', {}))}\n",
        "\n",
        "            Provide:\n",
        "            1. Key patterns and correlations\n",
        "            2. Logical inferences\n",
        "            3. Confidence assessment\n",
        "            4. Potential limitations\n",
        "            5. Actionable insights\n",
        "\n",
        "            Be analytical and evidence-based.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=600,\n",
        "                temperature=0.2\n",
        "            )\n",
        "\n",
        "            analysis = response.choices[0].message.content\n",
        "\n",
        "            # Calculate confidence based on data quality\n",
        "            confidence = 0.6\n",
        "            if data.get('kb_results'):\n",
        "                confidence += 0.2\n",
        "            if data.get('crypto_data'):\n",
        "                confidence += 0.15\n",
        "            if data.get('web_data'):\n",
        "                confidence += 0.15\n",
        "\n",
        "            confidence = min(confidence, 0.95)\n",
        "\n",
        "            result = {\n",
        "                \"analysis\": analysis,\n",
        "                \"confidence\": confidence,\n",
        "                \"reasoning_timestamp\": datetime.now().isoformat(),\n",
        "                \"data_sources\": len([k for k in ['kb_results', 'crypto_data', 'web_data'] if data.get(k)])\n",
        "            }\n",
        "\n",
        "            self.logger.log(self.name, f\"Analysis complete\",\n",
        "                           f\"Confidence: {confidence:.2f}\", \"SUCCESS\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.log(self.name, f\"Analysis error: {str(e)}\", status=\"ERROR\")\n",
        "            return {\"analysis\": \"Analysis unavailable\", \"confidence\": 0.0}\n",
        "\n",
        "class SynthesizerAgent:\n",
        "    \"\"\"Response synthesis and final answer generation agent\"\"\"\n",
        "\n",
        "    def __init__(self, logger: SystemLogger):\n",
        "        self.logger = logger\n",
        "        self.name = \"Synthesizer\"\n",
        "        self.client = client\n",
        "\n",
        "    def synthesize(self, all_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Synthesize final comprehensive response\"\"\"\n",
        "        self.logger.log(self.name, \"Synthesizing final response\", status=\"PROCESSING\")\n",
        "\n",
        "        try:\n",
        "            # Prepare synthesis prompt\n",
        "            prompt = f\"\"\"\n",
        "            As an expert synthesizer, create a comprehensive response using ALL available data:\n",
        "\n",
        "            Original Query: {all_data.get('query', 'N/A')}\n",
        "\n",
        "            Available Data:\n",
        "            - Knowledge Base: {len(all_data.get('kb_results', []))} relevant documents\n",
        "            - Crypto Analysis: {bool(all_data.get('crypto_data', {}))}\n",
        "            - Web Research: {bool(all_data.get('web_data', {}))}\n",
        "            - Reasoning Analysis: {bool(all_data.get('reasoning', {}))}\n",
        "\n",
        "            Create a response that:\n",
        "            1. Directly answers the query\n",
        "            2. Incorporates insights from all data sources\n",
        "            3. Provides specific examples and metrics\n",
        "            4. Includes confidence assessment\n",
        "            5. Cites relevant sources\n",
        "            6. Offers actionable recommendations\n",
        "\n",
        "            Make it comprehensive yet accessible.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=1000,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            final_response = response.choices[0].message.content\n",
        "\n",
        "            # Compile sources\n",
        "            sources = []\n",
        "            if all_data.get('kb_results'):\n",
        "                sources.extend([f\"Knowledge Base ({r['metadata']['source']})\" for r in all_data['kb_results']])\n",
        "            if all_data.get('crypto_data'):\n",
        "                sources.append(\"Etherscan API\")\n",
        "            if all_data.get('web_data'):\n",
        "                sources.extend([s['title'] for s in all_data['web_data'].get('sources', [])])\n",
        "\n",
        "            # Calculate overall confidence\n",
        "            confidences = []\n",
        "            if all_data.get('reasoning', {}).get('confidence'):\n",
        "                confidences.append(all_data['reasoning']['confidence'])\n",
        "            if all_data.get('web_data', {}).get('confidence'):\n",
        "                confidences.append(all_data['web_data']['confidence'])\n",
        "\n",
        "            overall_confidence = np.mean(confidences) if confidences else 0.7\n",
        "\n",
        "            result = {\n",
        "                \"response\": final_response,\n",
        "                \"sources\": list(set(sources)),\n",
        "                \"confidence\": overall_confidence,\n",
        "                \"synthesis_timestamp\": datetime.now().isoformat(),\n",
        "                \"data_integration_score\": len([k for k in ['kb_results', 'crypto_data', 'web_data', 'reasoning'] if all_data.get(k)])\n",
        "            }\n",
        "\n",
        "            self.logger.log(self.name, f\"Response synthesis complete\",\n",
        "                           f\"Integrated {len(sources)} sources, Confidence: {overall_confidence:.2f}\", \"SUCCESS\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.log(self.name, f\"Synthesis error: {str(e)}\", status=\"ERROR\")\n",
        "            return {\"response\": \"Synthesis unavailable\", \"confidence\": 0.0, \"sources\": []}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Vjzmc3GJLT"
      },
      "source": [
        "# MAIN AGENTIC RAG SYSTEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3qH-EvyL-lto"
      },
      "outputs": [],
      "source": [
        "class AgenticRAGSystem:\n",
        "    \"\"\"Main orchestrator for the multi-agent RAG system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.logger = SystemLogger()\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "\n",
        "        # Initialize agents\n",
        "        self.planner = PlannerAgent(self.logger)\n",
        "        self.retriever = RetrieverAgent(self.knowledge_base, self.logger)\n",
        "        self.crypto_agent = CryptoAgent(self.logger)\n",
        "        self.web_agent = WebAgent(self.logger)\n",
        "        self.reasoner = ReasonerAgent(self.logger)\n",
        "        self.synthesizer = SynthesizerAgent(self.logger)\n",
        "\n",
        "        self.query_history = []\n",
        "\n",
        "        print(\"ğŸ¤– Agentic RAG System initialized successfully!\")\n",
        "        print(f\"ğŸ“š Knowledge base loaded with {len(self.knowledge_base.documents)} documents\")\n",
        "\n",
        "    def process_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main query processing pipeline\"\"\"\n",
        "        self.logger.log(\"System\", f\"Processing query: {query[:50]}...\", status=\"PROCESSING\")\n",
        "\n",
        "        # Step 1: Planning\n",
        "        plan = self.planner.analyze_query(query)\n",
        "\n",
        "        # Step 2: Knowledge retrieval\n",
        "        kb_results = self.retriever.retrieve(query)\n",
        "\n",
        "        # Step 3: Conditional data gathering\n",
        "        crypto_data = {}\n",
        "        web_data = {}\n",
        "\n",
        "        if plan[\"requires_crypto_data\"]:\n",
        "            crypto_data = self.crypto_agent.analyze_blockchain_data(query)\n",
        "\n",
        "        if plan[\"requires_web_search\"]:\n",
        "            web_data = self.web_agent.search_web(query)\n",
        "\n",
        "        # Step 4: Reasoning (if needed)\n",
        "        reasoning = {}\n",
        "        if plan[\"requires_analysis\"]:\n",
        "            reasoning = self.reasoner.analyze({\n",
        "                \"query\": query,\n",
        "                \"kb_results\": kb_results,\n",
        "                \"crypto_data\": crypto_data,\n",
        "                \"web_data\": web_data\n",
        "            })\n",
        "\n",
        "        # Step 5: Synthesis\n",
        "        final_result = self.synthesizer.synthesize({\n",
        "            \"query\": query,\n",
        "            \"plan\": plan,\n",
        "            \"kb_results\": kb_results,\n",
        "            \"crypto_data\": crypto_data,\n",
        "            \"web_data\": web_data,\n",
        "            \"reasoning\": reasoning\n",
        "        })\n",
        "\n",
        "        # Store query history\n",
        "        self.query_history.append({\n",
        "            \"query\": query,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"result\": final_result,\n",
        "            \"plan\": plan\n",
        "        })\n",
        "\n",
        "        self.logger.log(\"System\", \"Query processing complete\",\n",
        "                       f\"Total time: {self.logger.get_summary()['total_time']}s\", \"SUCCESS\")\n",
        "\n",
        "        return {\n",
        "            \"response\": final_result[\"response\"],\n",
        "            \"sources\": final_result[\"sources\"],\n",
        "            \"confidence\": final_result[\"confidence\"],\n",
        "            \"plan\": plan,\n",
        "            \"system_logs\": self.logger.get_summary(),\n",
        "            \"processing_time\": self.logger.get_summary()[\"total_time\"]\n",
        "        }\n",
        "\n",
        "    def get_system_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system statistics and performance metrics\"\"\"\n",
        "        return {\n",
        "            \"total_queries\": len(self.query_history),\n",
        "            \"knowledge_base_size\": len(self.knowledge_base.documents),\n",
        "            \"avg_confidence\": np.mean([q[\"result\"][\"confidence\"] for q in self.query_history]) if self.query_history else 0,\n",
        "            \"agent_activity\": self.logger.get_summary()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3zJJn0rGOla"
      },
      "source": [
        "# INITIALIZE SYSTEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDqJJh4O-epB",
        "outputId": "a3495793-c24f-4702-982c-59c2f954ea9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Agentic RAG System initialized successfully!\n",
            "ğŸ“š Knowledge base loaded with 5 documents\n",
            "\n",
            "============================================================\n",
            "ğŸš€ AGENTIC RAG SYSTEM READY!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Create the main system\n",
        "rag_system = AgenticRAGSystem()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸš€ AGENTIC RAG SYSTEM READY!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMtdY4eB-bx9",
        "outputId": "a23ecec6-2f72-41bf-a761-6990eae869bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ§ª RUNNING SAMPLE QUERIES...\n",
            "============================================================\n",
            "\n",
            "ğŸ“ SAMPLE QUERY 1:\n",
            "\n",
            "ğŸ” PROCESSING QUERY: What is the current Ethereum gas price and how does it affect DeFi usage?\n",
            "--------------------------------------------------\n",
            "âš™ï¸ [21:02:02] System: Processing query: What is the current Ethereum gas price and how doe...\n",
            "âš™ï¸ [21:02:02] Planner: Analyzing query structure\n",
            "âœ… [21:02:02] Planner: Plan created - Complexity: high\n",
            "   â””â”€ Agents needed: retriever, crypto, web\n",
            "âš™ï¸ [21:02:02] Retriever: Searching knowledge base\n",
            "âœ… [21:02:02] Retriever: Retrieved 2 relevant documents\n",
            "   â””â”€ Avg similarity: 0.47\n",
            "âš™ï¸ [21:02:02] Crypto: Fetching blockchain data\n",
            "âŒ [21:02:02] Crypto: Error fetching gas prices: invalid literal for int() with base 10: '4.535442155'\n",
            "âœ… [21:02:02] Crypto: Blockchain analysis complete\n",
            "   â””â”€ Generated 1 insights\n",
            "âš™ï¸ [21:02:02] Web: Performing web research\n",
            "âŒ [21:02:04] Web: Web research error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "âš™ï¸ [21:02:04] Synthesizer: Synthesizing final response\n",
            "âŒ [21:02:06] Synthesizer: Synthesis error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "âœ… [21:02:06] System: Query processing complete\n",
            "   â””â”€ Total time: 7.18s\n",
            "\n",
            "ğŸ“Š RESPONSE:\n",
            "Synthesis unavailable\n",
            "\n",
            "ğŸ”— SOURCES: \n",
            "ğŸ“ˆ CONFIDENCE: 0.00\n",
            "â±ï¸ PROCESSING TIME: 7.18s\n",
            "\n",
            "ğŸ“ SAMPLE QUERY 2:\n",
            "\n",
            "ğŸ” PROCESSING QUERY: Analyze the correlation between cryptocurrency market trends and traditional finance\n",
            "--------------------------------------------------\n",
            "âš™ï¸ [21:02:08] System: Processing query: Analyze the correlation between cryptocurrency mar...\n",
            "âš™ï¸ [21:02:08] Planner: Analyzing query structure\n",
            "âœ… [21:02:08] Planner: Plan created - Complexity: complex\n",
            "   â””â”€ Agents needed: retriever, crypto, web, reasoner\n",
            "âš™ï¸ [21:02:08] Retriever: Searching knowledge base\n",
            "âœ… [21:02:08] Retriever: Retrieved 1 relevant documents\n",
            "   â””â”€ Avg similarity: 0.32\n",
            "âš™ï¸ [21:02:08] Crypto: Fetching blockchain data\n",
            "âŒ [21:02:08] Crypto: Error fetching gas prices: invalid literal for int() with base 10: '4.564190106'\n",
            "âœ… [21:02:08] Crypto: Blockchain analysis complete\n",
            "   â””â”€ Generated 1 insights\n",
            "âš™ï¸ [21:02:08] Web: Performing web research\n",
            "âŒ [21:02:10] Web: Web research error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "âš™ï¸ [21:02:10] Reasoner: Performing logical analysis\n",
            "âŒ [21:02:11] Reasoner: Analysis error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "âš™ï¸ [21:02:11] Synthesizer: Synthesizing final response\n",
            "âŒ [21:02:13] Synthesizer: Synthesis error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "âœ… [21:02:13] System: Query processing complete\n",
            "   â””â”€ Total time: 14.37s\n",
            "\n",
            "ğŸ“Š RESPONSE:\n",
            "Synthesis unavailable\n",
            "\n",
            "ğŸ”— SOURCES: \n",
            "ğŸ“ˆ CONFIDENCE: 0.00\n",
            "â±ï¸ PROCESSING TIME: 14.37s\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE USAGE AND TESTING\n",
        "\n",
        "def demo_query(query: str):\n",
        "    \"\"\"Demonstrate the system with a sample query\"\"\"\n",
        "    print(f\"\\nğŸ” PROCESSING QUERY: {query}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    result = rag_system.process_query(query)\n",
        "\n",
        "    print(f\"\\nğŸ“Š RESPONSE:\")\n",
        "    print(result[\"response\"])\n",
        "    print(f\"\\nğŸ”— SOURCES: {', '.join(result['sources'])}\")\n",
        "    print(f\"ğŸ“ˆ CONFIDENCE: {result['confidence']:.2f}\")\n",
        "    print(f\"â±ï¸ PROCESSING TIME: {result['processing_time']:.2f}s\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# Test with sample queries\n",
        "sample_queries = [\n",
        "    \"What is the current Ethereum gas price and how does it affect DeFi usage?\",\n",
        "    \"Analyze the correlation between cryptocurrency market trends and traditional finance\",\n",
        "    \"Explain Layer 2 scaling solutions and their impact on blockchain adoption\",\n",
        "    \"How do autonomous agents work in AI systems?\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ§ª RUNNING SAMPLE QUERIES...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, query in enumerate(sample_queries[:2], 1):  # Test first 2 queries\n",
        "    print(f\"\\nğŸ“ SAMPLE QUERY {i}:\")\n",
        "    demo_query(query)\n",
        "    time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DtgJFuZK-WQE"
      },
      "outputs": [],
      "source": [
        "# INTERACTIVE GRADIO INTERFACE\n",
        "\n",
        "def process_query_interface(query):\n",
        "    \"\"\"Gradio interface function\"\"\"\n",
        "    if not query.strip():\n",
        "        return \"Please enter a query to process.\", \"No sources\", \"0.00\", \"0.00\"\n",
        "\n",
        "    try:\n",
        "        result = rag_system.process_query(query)\n",
        "        return (\n",
        "            result[\"response\"],\n",
        "            \", \".join(result[\"sources\"]) if result[\"sources\"] else \"No sources\",\n",
        "            f\"{result['confidence']:.2f}\",\n",
        "            f\"{result['processing_time']:.2f}s\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"Error processing query: {str(e)}\", \"Error\", \"0.00\", \"0.00\"\n",
        "\n",
        "def get_system_statistics():\n",
        "    \"\"\"Get formatted system statistics\"\"\"\n",
        "    stats = rag_system.get_system_stats()\n",
        "    return f\"\"\"\n",
        "    ğŸ“Š SYSTEM STATISTICS:\n",
        "    â€¢ Total Queries Processed: {stats['total_queries']}\n",
        "    â€¢ Knowledge Base Size: {stats['knowledge_base_size']} documents\n",
        "    â€¢ Average Confidence: {stats['avg_confidence']:.2f}\n",
        "    â€¢ Total Agent Activities: {stats['agent_activity']['total_logs']}\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "nGtpX3q-3j58",
        "outputId": "0cf505ea-9c73-4e28-ac21-fed99a1aa906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://640c59f0973427a2cd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://640c59f0973427a2cd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://640c59f0973427a2cd.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "class MockAgenticRAG:\n",
        "    \"\"\"Mock implementation of an Agentic RAG system for demonstration\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = {\n",
        "            \"ethereum\": \"Ethereum is a decentralized blockchain platform with smart contract functionality.\",\n",
        "            \"defi\": \"DeFi (Decentralized Finance) refers to financial services built on blockchain networks.\",\n",
        "            \"gas fees\": \"Gas fees are transaction costs on the Ethereum network, varying based on network congestion.\",\n",
        "            \"layer 2\": \"Layer 2 solutions are scaling technologies built on top of existing blockchains.\",\n",
        "            \"ai agents\": \"AI agents are autonomous systems that can perform tasks and make decisions independently.\",\n",
        "            \"bitcoin\": \"Bitcoin is the first and largest cryptocurrency by market capitalization.\",\n",
        "            \"blockchain\": \"Blockchain is a distributed ledger technology that maintains a continuously growing list of records.\",\n",
        "            \"smart contracts\": \"Smart contracts are self-executing contracts with terms directly written into code.\",\n",
        "            \"nft\": \"NFTs (Non-Fungible Tokens) are unique digital assets stored on blockchain networks.\",\n",
        "            \"mining\": \"Mining is the process of validating transactions and adding them to the blockchain.\",\n",
        "            \"wallet\": \"A cryptocurrency wallet is a digital tool for storing and managing cryptocurrencies.\",\n",
        "            \"staking\": \"Staking involves holding cryptocurrencies to support network operations and earn rewards.\",\n",
        "            \"consensus\": \"Consensus mechanisms are protocols that ensure network participants agree on the blockchain state.\",\n",
        "            \"decentralization\": \"Decentralization refers to the distribution of authority away from central control.\",\n",
        "            \"tokenomics\": \"Tokenomics is the study of the economic models and incentives of cryptocurrency tokens.\"\n",
        "        }\n",
        "\n",
        "    def search_knowledge_base(self, query):\n",
        "        \"\"\"Simulate knowledge base search\"\"\"\n",
        "        results = []\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        for key, value in self.knowledge_base.items():\n",
        "            if key in query_lower:\n",
        "                results.append(f\"KB: {value}\")\n",
        "\n",
        "        return results if results else [\"KB: General information available in knowledge base\"]\n",
        "\n",
        "    def web_search(self, query, enabled=True):\n",
        "        \"\"\"Simulate web search\"\"\"\n",
        "        if not enabled:\n",
        "            return []\n",
        "\n",
        "        # Simulate web search results\n",
        "        web_results = [\n",
        "            f\"Web: Latest information about {query.split()[0] if query.split() else 'topic'}\",\n",
        "            f\"Web: Current market data and trends related to your query\",\n",
        "            f\"Web: Recent news and developments\"\n",
        "        ]\n",
        "        return web_results[:2]  # Return top 2 results\n",
        "\n",
        "    def api_calls(self, query):\n",
        "        \"\"\"Simulate API calls\"\"\"\n",
        "        # Simulate different API responses based on query content\n",
        "        if \"ethereum\" in query.lower() or \"gas\" in query.lower():\n",
        "            return [\"API: Current ETH price: $2,145\", \"API: Average gas price: 25 gwei\"]\n",
        "        elif \"defi\" in query.lower():\n",
        "            return [\"API: Total Value Locked: $89.2B\", \"API: Top protocols: Uniswap, Aave, Compound\"]\n",
        "        else:\n",
        "            return [\"API: Real-time data retrieved\", \"API: Market metrics updated\"]\n",
        "\n",
        "    def process_query(self, query, model=\"gpt-4\", max_tokens=2000, temperature=0.7, web_search_enabled=True):\n",
        "        \"\"\"Main query processing function\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Simulate agent activities\n",
        "        logs = []\n",
        "        sources = []\n",
        "\n",
        "        # Agent 1: Knowledge Base Search\n",
        "        logs.append(\"Agent 1: Searching knowledge base...\")\n",
        "        kb_results = self.search_knowledge_base(query)\n",
        "        sources.extend(kb_results)\n",
        "        logs.append(f\"Agent 1: Found {len(kb_results)} KB results\")\n",
        "\n",
        "        # Agent 2: Web Search\n",
        "        if web_search_enabled:\n",
        "            logs.append(\"Agent 2: Performing web search...\")\n",
        "            web_results = self.web_search(query, web_search_enabled)\n",
        "            sources.extend(web_results)\n",
        "            logs.append(f\"Agent 2: Retrieved {len(web_results)} web results\")\n",
        "\n",
        "        # Agent 3: API Calls\n",
        "        logs.append(\"Agent 3: Making API calls...\")\n",
        "        api_results = self.api_calls(query)\n",
        "        sources.extend(api_results)\n",
        "        logs.append(f\"Agent 3: Completed {len(api_results)} API calls\")\n",
        "\n",
        "        # Simulate processing time\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # Generate response based on query\n",
        "        response = self.generate_response(query, sources, model, temperature)\n",
        "\n",
        "        # Calculate metrics\n",
        "        processing_time = time.time() - start_time\n",
        "        confidence = random.randint(75, 95)\n",
        "        tokens_used = random.randint(800, int(max_tokens * 0.8))\n",
        "\n",
        "        return {\n",
        "            'response': response,\n",
        "            'sources': sources,\n",
        "            'confidence': confidence,\n",
        "            'logs': logs,\n",
        "            'processing_time': processing_time,\n",
        "            'tokens_used': tokens_used\n",
        "        }\n",
        "\n",
        "    def generate_response(self, query, sources, model, temperature):\n",
        "        \"\"\"Generate a contextual response based on the query and sources\"\"\"\n",
        "\n",
        "        # Basic response generation based on query content\n",
        "        if \"ethereum\" in query.lower() and \"gas\" in query.lower():\n",
        "            response = \"\"\"Based on current data analysis:\n",
        "\n",
        "**Ethereum Gas Fees Analysis:**\n",
        "- Current average gas price is around 25 gwei\n",
        "- Network congestion affects fee volatility\n",
        "- Layer 2 solutions offer significant cost reduction\n",
        "\n",
        "**Impact on DeFi Adoption:**\n",
        "- High gas fees create barriers for small transactions\n",
        "- Layer 2 scaling solutions are gaining traction\n",
        "- Users are migrating to more cost-effective alternatives\n",
        "\n",
        "This analysis combines real-time API data, knowledge base information, and current market trends.\"\"\"\n",
        "\n",
        "        elif \"defi\" in query.lower():\n",
        "            response = \"\"\"**DeFi Market Analysis:**\n",
        "\n",
        "Current DeFi landscape shows:\n",
        "- Total Value Locked (TVL): $89.2B across protocols\n",
        "- Leading protocols: Uniswap, Aave, Compound\n",
        "- Growing adoption of Layer 2 solutions\n",
        "- Increased institutional participation\n",
        "\n",
        "Market trends indicate continued growth despite volatility, with focus on user experience improvements and gas optimization.\"\"\"\n",
        "\n",
        "        elif \"layer 2\" in query.lower():\n",
        "            response = \"\"\"**Layer 2 Scaling Solutions Comparison:**\n",
        "\n",
        "Key solutions analyzed:\n",
        "- **Polygon**: High throughput, low fees\n",
        "- **Arbitrum**: Optimistic rollup technology\n",
        "- **Optimism**: Fast withdrawal times\n",
        "- **StarkNet**: Zero-knowledge proofs\n",
        "\n",
        "Each solution offers different trade-offs between security, speed, and cost. Selection depends on specific use case requirements.\"\"\"\n",
        "\n",
        "        elif \"ai agent\" in query.lower():\n",
        "            response = \"\"\"**AI Agent Systems Development:**\n",
        "\n",
        "Current trends in AI agents:\n",
        "- Autonomous decision-making capabilities\n",
        "- Multi-modal interaction support\n",
        "- Integration with blockchain systems\n",
        "- Enhanced reasoning and planning\n",
        "\n",
        "These systems are becoming increasingly sophisticated, with applications in finance, automation, and decentralized systems.\"\"\"\n",
        "\n",
        "        else:\n",
        "            response = f\"\"\"**Analysis Results for: \"{query}\"**\n",
        "\n",
        "Based on comprehensive data analysis from multiple sources:\n",
        "\n",
        "Our agentic system has processed your query through:\n",
        "1. Knowledge base search\n",
        "2. Real-time web data retrieval\n",
        "3. API integrations for current metrics\n",
        "\n",
        "The system has synthesized information from {len(sources)} sources to provide this comprehensive response.\n",
        "\n",
        "Key insights and recommendations have been compiled based on the most recent and relevant data available.\"\"\"\n",
        "\n",
        "        return response\n",
        "\n",
        "# Initialize the RAG system\n",
        "rag_system = MockAgenticRAG()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Agentic RAG System\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ğŸ¤– Agentic RAG System - Multi-Source Intelligence Platform\")\n",
        "    gr.Markdown(\"*Advanced AI system with autonomous agents for comprehensive query processing*\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            query_input = gr.Textbox(\n",
        "                label=\"Enter your query:\",\n",
        "                placeholder=\"e.g., 'Analyze the current Ethereum gas fees and their impact on DeFi adoption'\",\n",
        "                lines=3\n",
        "            )\n",
        "            process_btn = gr.Button(\"ğŸš€ Process Query\", variant=\"primary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        \"What is the current Ethereum gas price and network status?\",\n",
        "                        \"Analyze DeFi market trends and total value locked\",\n",
        "                        \"Compare Layer 2 scaling solutions for Ethereum\",\n",
        "                        \"Explain the latest developments in AI agent systems\"\n",
        "                    ],\n",
        "                    inputs=query_input\n",
        "                )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            stats_btn = gr.Button(\"ğŸ“Š View System Stats\")\n",
        "            stats_output = gr.Textbox(label=\"System Statistics\", lines=8)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            response_output = gr.Textbox(label=\"ğŸ¤– Agent Response\", lines=12)\n",
        "        with gr.Column():\n",
        "            sources_output = gr.Textbox(label=\"ğŸ”— Sources\", lines=8)\n",
        "            confidence_output = gr.Textbox(label=\"ğŸ¯ Confidence & Metrics\", lines=4)\n",
        "\n",
        "    # Additional components for enhanced functionality\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            agent_logs = gr.Textbox(label=\"ğŸ” Agent Activity Log\", lines=6)\n",
        "        with gr.Column():\n",
        "            performance_metrics = gr.Textbox(label=\"âš¡ Performance Metrics\", lines=6)\n",
        "\n",
        "    # Advanced options (collapsible)\n",
        "    with gr.Accordion(\"âš™ï¸ Advanced Options\", open=False):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                model_choice = gr.Dropdown(\n",
        "                    choices=[\"gpt-4\", \"gpt-3.5-turbo\", \"claude-3\", \"llama-2\"],\n",
        "                    label=\"Select Model\",\n",
        "                    value=\"gpt-4\"\n",
        "                )\n",
        "                max_tokens = gr.Slider(\n",
        "                    minimum=100,\n",
        "                    maximum=4000,\n",
        "                    value=2000,\n",
        "                    label=\"Max Tokens\"\n",
        "                )\n",
        "            with gr.Column():\n",
        "                temperature = gr.Slider(\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    value=0.7,\n",
        "                    label=\"Temperature\"\n",
        "                )\n",
        "                enable_web_search = gr.Checkbox(\n",
        "                    label=\"Enable Web Search\",\n",
        "                    value=True\n",
        "                )\n",
        "\n",
        "    # Event handlers\n",
        "    def process_query_handler(query, model, max_tokens, temperature, web_search):\n",
        "        \"\"\"Process the user query through the agentic RAG system\"\"\"\n",
        "        if not query.strip():\n",
        "            return \"Please enter a query to process.\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        try:\n",
        "            result = rag_system.process_query(\n",
        "                query=query,\n",
        "                model=model,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                web_search_enabled=web_search\n",
        "            )\n",
        "\n",
        "            # Format outputs\n",
        "            response = result['response']\n",
        "            sources = \"\\n\".join([f\"â€¢ {source}\" for source in result['sources']])\n",
        "            confidence = f\"Confidence: {result['confidence']}%\\nReliability: {'High' if result['confidence'] > 85 else 'Medium'}\"\n",
        "            logs = \"\\n\".join([f\"[{datetime.now().strftime('%H:%M:%S')}] {log}\" for log in result['logs']])\n",
        "            metrics = f\"\"\"Processing Time: {result['processing_time']:.2f}s\n",
        "Tokens Used: {result['tokens_used']:,}\n",
        "Sources Queried: {len(result['sources'])}\n",
        "Model: {model}\n",
        "Temperature: {temperature}\"\"\"\n",
        "\n",
        "            return response, sources, confidence, logs, metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error processing query: {str(e)}\"\n",
        "            return error_msg, \"\", \"\", error_msg, \"\"\n",
        "\n",
        "    def get_system_stats():\n",
        "        \"\"\"Get current system statistics\"\"\"\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M UTC')\n",
        "        stats = f\"\"\"System Status: Online âœ…\n",
        "Active Agents: 3/3\n",
        "Knowledge Base: 1.2M documents\n",
        "API Endpoints: 15 active\n",
        "Cache Hit Rate: {random.randint(85, 95)}%\n",
        "Avg Response Time: {random.uniform(1.5, 2.5):.1f}s\n",
        "Success Rate: {random.uniform(95, 99):.1f}%\n",
        "Last Updated: {current_time}\"\"\"\n",
        "        return stats\n",
        "\n",
        "    # Wire up the event handlers\n",
        "    process_btn.click(\n",
        "        fn=process_query_handler,\n",
        "        inputs=[query_input, model_choice, max_tokens, temperature, enable_web_search],\n",
        "        outputs=[response_output, sources_output, confidence_output, agent_logs, performance_metrics]\n",
        "    )\n",
        "\n",
        "    stats_btn.click(\n",
        "        fn=get_system_stats,\n",
        "        outputs=stats_output\n",
        "    )\n",
        "\n",
        "    # Auto-load stats on startup\n",
        "    demo.load(\n",
        "        fn=get_system_stats,\n",
        "        outputs=stats_output\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
